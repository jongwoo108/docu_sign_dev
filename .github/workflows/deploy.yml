name: "CI/CD to EKS"

on:
  push:
    branches: ["main"]
    paths-ignore:
      - "README.md"
  workflow_dispatch: {}

concurrency:
  group: production-deploy
  cancel-in-progress: true

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
      ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      K8S_NAMESPACE: ${{ secrets.K8S_NAMESPACE }}
      CONTAINER_NAME: web

    steps:
      - uses: actions/checkout@v4
      - name: Set kubeconfig path
        run: echo "KUBECONFIG_PATH=$RUNNER_TEMP/kubeconfig" >> "$GITHUB_ENV"

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Debug - Check AWS identity and permissions
        run: |
          echo "=== AWS Identity ==="
          aws sts get-caller-identity
          echo ""
          echo "=== Role ARN from secrets ==="
          echo "AWS_ROLE_TO_ASSUME: ${{ secrets.AWS_ROLE_TO_ASSUME }}"
          echo ""
          echo "=== EKS Cluster Info ==="
          aws eks describe-cluster --name "${EKS_CLUSTER_NAME}" --region "${AWS_REGION}" --query 'cluster.{name:name,status:status,version:version,arn:arn}' || echo "Cannot describe cluster"
          echo ""
          echo "=== Environment Variables ==="
          echo "AWS_REGION: ${AWS_REGION}"
          echo "EKS_CLUSTER_NAME: ${EKS_CLUSTER_NAME}"
          echo "K8S_NAMESPACE: ${K8S_NAMESPACE}"

      - name: Login to Amazon ECR
        run: |
          aws ecr get-login-password --region "${AWS_REGION}" \
          | docker login --username AWS --password-stdin \
            "${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"

      - name: Build & Push Docker image
        id: push-image
        run: |
          set -euxo pipefail
          REPO="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${ECR_REPOSITORY}"
          IMAGE_TAG="${GITHUB_SHA::7}"
          docker build -f docker/Dockerfile -t "${REPO}:${IMAGE_TAG}" .
          docker push "${REPO}:${IMAGE_TAG}"
          echo "IMAGE=${REPO}:${IMAGE_TAG}" >> "$GITHUB_ENV"

      - name: Install kubectl (if missing)
        run: |
          if ! command -v kubectl >/dev/null; then
            curl -sSL -o kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
            chmod +x kubectl && sudo mv kubectl /usr/local/bin/kubectl
          fi

      - name: Update kubeconfig
        id: kubeconfig
        run: |
          set -euxo pipefail
          echo "=== Updating kubeconfig ==="
          aws eks update-kubeconfig \
            --name "${EKS_CLUSTER_NAME}" \
            --region "${AWS_REGION}" \
            --kubeconfig "${KUBECONFIG_PATH}"
          echo ""
          echo "=== kubectl version ==="
          kubectl --kubeconfig "${KUBECONFIG_PATH}" version --client
          echo ""
          echo "=== Testing basic kubectl access ==="
          kubectl --kubeconfig "${KUBECONFIG_PATH}" get nodes || echo "Cannot access nodes (this might be expected if role lacks permissions)"
          echo "KUBECONFIG_READY=true" >> "$GITHUB_ENV"

      - name: Debug - Check current aws-auth ConfigMap
        if: env.KUBECONFIG_READY == 'true'
        run: |
          echo "=== Current aws-auth ConfigMap ==="
          kubectl --kubeconfig "${KUBECONFIG_PATH}" get configmap aws-auth -n kube-system -o yaml || echo "Cannot read aws-auth ConfigMap"
        continue-on-error: true

      - name: RBAC sanity check
        if: env.KUBECONFIG_READY == 'true'
        run: |
          echo "=== RBAC Permission Check ==="
          kubectl --kubeconfig "${KUBECONFIG_PATH}" -n "${K8S_NAMESPACE}" auth can-i patch deployment || echo "No permission to patch deployments"
          kubectl --kubeconfig "${KUBECONFIG_PATH}" -n "${K8S_NAMESPACE}" auth can-i get pods || echo "No permission to get pods"
          kubectl --kubeconfig "${KUBECONFIG_PATH}" -n "${K8S_NAMESPACE}" auth can-i list deployments || echo "No permission to list deployments"
        continue-on-error: true

      - name: Resolve container name (auto-detect)
        if: env.KUBECONFIG_READY == 'true'
        run: |
          CN="${CONTAINER_NAME}"
          if [ -n "${{ secrets.CONTAINER_NAME }}" ]; then
            CN="${{ secrets.CONTAINER_NAME }}"
          else
            CN="$(kubectl --kubeconfig "${KUBECONFIG_PATH}" -n "${K8S_NAMESPACE}" \
                  get deploy/documenso -o jsonpath='{.spec.template.spec.containers[0].name}' 2>/dev/null || echo 'web')"
          fi
          echo "USING_CONTAINER=${CN}" >> "$GITHUB_ENV"
          echo "Using container: ${CN}"
        continue-on-error: true

      - name: Rolling update
        if: env.KUBECONFIG_READY == 'true'
        run: |
          set -euxo pipefail
          echo "=== Starting rolling update ==="
          echo "Image: ${IMAGE}"
          echo "Container: ${USING_CONTAINER}"
          kubectl --kubeconfig "${KUBECONFIG_PATH}" -n "${K8S_NAMESPACE}" \
            set image deploy/documenso "${USING_CONTAINER}"="${IMAGE}"
          kubectl --kubeconfig "${KUBECONFIG_PATH}" -n "${K8S_NAMESPACE}" \
            rollout status deploy/documenso --timeout=180s
        continue-on-error: true

      - name: On failure â†’ Rollback
        if: failure() && env.KUBECONFIG_READY == 'true'
        run: |
          echo "=== Rolling back deployment ==="
          kubectl --kubeconfig "${KUBECONFIG_PATH}" -n "${K8S_NAMESPACE}" \
            rollout undo deploy/documenso
        continue-on-error: true
